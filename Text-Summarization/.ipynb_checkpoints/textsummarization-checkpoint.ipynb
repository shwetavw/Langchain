{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3adbd42e-8da2-4ed3-8cb1-56702356e1e3",
   "metadata": {},
   "source": [
    "# Different way of Text Summarization using Langchain and Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72b9ee-7739-4f1b-84e9-08fcf5644e67",
   "metadata": {},
   "source": [
    "#### Set API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cef8cb46-a399-4e2b-b543-5152c7703d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "google_api_key = \"{Google_API_Key}\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = google_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21368f42-a168-4cbf-8d5c-23b8927b3ce8",
   "metadata": {},
   "source": [
    "#### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10343e1b-9ac3-4f49-aa82-1601c00b2253",
   "metadata": {},
   "source": [
    "Install LangChain's Python library, langchain and langchain_community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a518c434-a87c-4dac-bf33-70b865cc3e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98427bf8-a7d8-4f39-ae04-b707109114dc",
   "metadata": {},
   "source": [
    "Install LangChain's integration package for Gemini, langchain-google-genai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5a9deb90-1f39-489c-b5ef-00fffd5d18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6accc5e-3dbb-4533-a8b3-7d1e15431bb8",
   "metadata": {},
   "source": [
    "### Basic Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "992a01c1-ec68-40a9-872e-59f0de0d3821",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Today, AI and ML are at the forefront of change and driving a technological renaissance, redefining how humanity approaches its greatest challenges. The rapid progress has made AI an indispensable tool for global transformation. As NASSCOM and BCG forecast, India's AI market is on a meteoric rise, expected to reach $17 billion by 2027, becoming a key driver of economic growth and innovation.\n",
    "\n",
    "However, there are challenges. India faces a 51% demand-supply gap in AI and ML professionals, with a demand of over 629,000 specialists but a supply of only 416,000. This gap presents a tremendous opportunity with 10x more projected jobs and 4x more higher salaries available to those who upskill in AI and ML. Recognising this demand, Emeritus and IITM Pravartak (the technological hub of IIT Madras) have launched the Advanced Certificate Programme in Applied Artificial Intelligence and Machine Learning.\n",
    "\n",
    "This programme is tailored for professionals eager to leverage cutting-edge technologies to drive innovation and tackle complex problems. This unique programme offers participants a competitive edge in the industry, featuring rigorous instruction from distinguished IIT faculty and globally renowned AI and ML experts. The faculty lineup includes Professor C. Chandra Sekhar, former Head of the CSE Department at IIT Madras (2019–22), and Professor Dileep A.D., Head of the CSE Department at IIT Dharwad and a research scholar at IIT Madras. These award-winning educators have published numerous research papers in prestigious international journals, making them ideal mentors for this transformative journey. The programme further combines IIT faculty teachings with live sessions led by industry experts, delving into real-world applications and tools. Participants will also engage in hands-on labs designed to cultivate practical expertise, ensuring that they graduate as well-rounded AI and ML professionals ready to meet the demands of this rapidly growing field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d3731229-e2cd-49ce-9a4b-1af5aca8c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# Instantiate Model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7, top_p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ef147c07-c9a3-481a-bbcc-3f39df0e5540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check total tokens\n",
    "llm.get_num_tokens(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "494acea3-214b-4af5-a768-b35154f53396",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = llm([\n",
    "    SystemMessage(role=\"system\", content=\"You are a skilled assistant with expertise in summarizing text\"),\n",
    "    HumanMessage(role=\"user\", content=f'Please provide a concise summary of the following \\n TEXT: {input_text}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b44a7284-2c8d-4712-8ca9-35fb53b05f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"AI and machine learning are transforming industries, and India's AI market is projected to reach $17 billion by 2027.  However, a significant skills gap exists. To address this, Emeritus and IITM Pravartak have launched an Advanced Certificate Programme in Applied AI and ML.  The program, taught by renowned IIT faculty and industry experts, offers rigorous training and hands-on experience to equip professionals with the skills needed to succeed in this burgeoning field.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-36336c6d-0435-4d7f-bb16-a50434f76932-0', usage_metadata={'input_tokens': 416, 'output_tokens': 97, 'total_tokens': 513, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model response\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "24750396-9920-445d-93b2-8576181dcd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI and machine learning are transforming industries, and India's AI market is projected to reach $17 billion by 2027.  However, a significant skills gap exists. To address this, Emeritus and IITM Pravartak have launched an Advanced Certificate Programme in Applied AI and ML.  The program, taught by renowned IIT faculty and industry experts, offers rigorous training and hands-on experience to equip professionals with the skills needed to succeed in this burgeoning field.\n"
     ]
    }
   ],
   "source": [
    "# get content\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34507f2-798f-49bc-b003-dc62812639f0",
   "metadata": {},
   "source": [
    "### Prompt Template Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c98a8b1c-649c-4f93-b0d5-9bf51a56d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "82ff2177-538c-4794-854a-10c5c5aa210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Today, AI and ML are at the forefront of change and driving a technological renaissance, redefining how humanity approaches its greatest challenges. The rapid progress has made AI an indispensable tool for global transformation. As NASSCOM and BCG forecast, India's AI market is on a meteoric rise, expected to reach $17 billion by 2027, becoming a key driver of economic growth and innovation.\n",
    "\n",
    "However, there are challenges. India faces a 51% demand-supply gap in AI and ML professionals, with a demand of over 629,000 specialists but a supply of only 416,000. This gap presents a tremendous opportunity with 10x more projected jobs and 4x more higher salaries available to those who upskill in AI and ML. Recognising this demand, Emeritus and IITM Pravartak (the technological hub of IIT Madras) have launched the Advanced Certificate Programme in Applied Artificial Intelligence and Machine Learning.\n",
    "\n",
    "This programme is tailored for professionals eager to leverage cutting-edge technologies to drive innovation and tackle complex problems. This unique programme offers participants a competitive edge in the industry, featuring rigorous instruction from distinguished IIT faculty and globally renowned AI and ML experts. The faculty lineup includes Professor C. Chandra Sekhar, former Head of the CSE Department at IIT Madras (2019–22), and Professor Dileep A.D., Head of the CSE Department at IIT Dharwad and a research scholar at IIT Madras. These award-winning educators have published numerous research papers in prestigious international journals, making them ideal mentors for this transformative journey. The programme further combines IIT faculty teachings with live sessions led by industry experts, delving into real-world applications and tools. Participants will also engage in hands-on labs designed to cultivate practical expertise, ensuring that they graduate as well-rounded AI and ML professionals ready to meet the demands of this rapidly growing field.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "358d9c3c-2405-4e40-ad34-4620b1d475c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWrite a concise summary of the following text : `\\nToday, AI and ML are at the forefront of change and driving a technological renaissance, redefining how humanity approaches its greatest challenges. The rapid progress has made AI an indispensable tool for global transformation. As NASSCOM and BCG forecast, India's AI market is on a meteoric rise, expected to reach $17 billion by 2027, becoming a key driver of economic growth and innovation.\\n\\nHowever, there are challenges. India faces a 51% demand-supply gap in AI and ML professionals, with a demand of over 629,000 specialists but a supply of only 416,000. This gap presents a tremendous opportunity with 10x more projected jobs and 4x more higher salaries available to those who upskill in AI and ML. Recognising this demand, Emeritus and IITM Pravartak (the technological hub of IIT Madras) have launched the Advanced Certificate Programme in Applied Artificial Intelligence and Machine Learning.\\n\\nThis programme is tailored for professionals eager to leverage cutting-edge technologies to drive innovation and tackle complex problems. This unique programme offers participants a competitive edge in the industry, featuring rigorous instruction from distinguished IIT faculty and globally renowned AI and ML experts. The faculty lineup includes Professor C. Chandra Sekhar, former Head of the CSE Department at IIT Madras (2019–22), and Professor Dileep A.D., Head of the CSE Department at IIT Dharwad and a research scholar at IIT Madras. These award-winning educators have published numerous research papers in prestigious international journals, making them ideal mentors for this transformative journey. The programme further combines IIT faculty teachings with live sessions led by industry experts, delving into real-world applications and tools. Participants will also engage in hands-on labs designed to cultivate practical expertise, ensuring that they graduate as well-rounded AI and ML professionals ready to meet the demands of this rapidly growing field.\\n`\\nTranslate the summary in French.\\n\""
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define prompt template and Instantiate prompt\n",
    "template=\"\"\"\n",
    "Write a concise summary of the following text : `{input_text}`\n",
    "Translate the summary in {language}.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=['input_text','language'],\n",
    "    template=template\n",
    ")\n",
    "prompt.format(input_text=input_text,language='French')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cecac6d8-3b21-4eff-ab6c-9100fc070688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Instantiate model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7, top_p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "96864024-7070-4191-8c8c-43e9dfe077ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check total tokens\n",
    "llm.get_num_tokens(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4c4e1182-6d7a-42f6-991b-4634f90c5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "summary=llm_chain.invoke({'input_text':input_text,'language':'Fench'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a6b1dcdf-2355-4339-be8d-ab554c47b49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': \"\\nToday, AI and ML are at the forefront of change and driving a technological renaissance, redefining how humanity approaches its greatest challenges. The rapid progress has made AI an indispensable tool for global transformation. As NASSCOM and BCG forecast, India's AI market is on a meteoric rise, expected to reach $17 billion by 2027, becoming a key driver of economic growth and innovation.\\n\\nHowever, there are challenges. India faces a 51% demand-supply gap in AI and ML professionals, with a demand of over 629,000 specialists but a supply of only 416,000. This gap presents a tremendous opportunity with 10x more projected jobs and 4x more higher salaries available to those who upskill in AI and ML. Recognising this demand, Emeritus and IITM Pravartak (the technological hub of IIT Madras) have launched the Advanced Certificate Programme in Applied Artificial Intelligence and Machine Learning.\\n\\nThis programme is tailored for professionals eager to leverage cutting-edge technologies to drive innovation and tackle complex problems. This unique programme offers participants a competitive edge in the industry, featuring rigorous instruction from distinguished IIT faculty and globally renowned AI and ML experts. The faculty lineup includes Professor C. Chandra Sekhar, former Head of the CSE Department at IIT Madras (2019–22), and Professor Dileep A.D., Head of the CSE Department at IIT Dharwad and a research scholar at IIT Madras. These award-winning educators have published numerous research papers in prestigious international journals, making them ideal mentors for this transformative journey. The programme further combines IIT faculty teachings with live sessions led by industry experts, delving into real-world applications and tools. Participants will also engage in hands-on labs designed to cultivate practical expertise, ensuring that they graduate as well-rounded AI and ML professionals ready to meet the demands of this rapidly growing field.\\n\",\n",
       " 'language': 'Fench',\n",
       " 'text': \"AI and machine learning are transforming industries, and India's AI market is projected to reach $17 billion by 2027.  However, a significant skills gap exists, with demand for AI/ML professionals far exceeding supply. To address this, Emeritus and IITM Pravartak have launched an advanced certificate program in applied AI and ML, taught by renowned IIT faculty and industry experts, offering rigorous training and practical experience to prepare professionals for this burgeoning field.\\n\\n\\nFrench Translation:\\n\\nL'intelligence artificielle et l'apprentissage automatique transforment les industries, et le marché indien de l'IA devrait atteindre 17 milliards de dollars d'ici 2027. Cependant, il existe un écart important en matière de compétences, la demande de professionnels de l'IA/ML dépassant largement l'offre. Pour y remédier, Emeritus et IITM Pravartak ont lancé un programme de certificat avancé en IA et ML appliquées, dispensé par des professeurs renommés de l'IIT et des experts de l'industrie, offrant une formation rigoureuse et une expérience pratique pour préparer les professionnels à ce domaine en plein essor.\"}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model response\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "45762d7d-4d70-4672-a734-2a71e3ee5dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI and machine learning are transforming industries, and India's AI market is projected to reach $17 billion by 2027.  However, a significant skills gap exists, with demand for AI/ML professionals far exceeding supply. To address this, Emeritus and IITM Pravartak have launched an advanced certificate program in applied AI and ML, taught by renowned IIT faculty and industry experts, offering rigorous training and practical experience to prepare professionals for this burgeoning field.\\n\\n\\nFrench Translation:\\n\\nL'intelligence artificielle et l'apprentissage automatique transforment les industries, et le marché indien de l'IA devrait atteindre 17 milliards de dollars d'ici 2027. Cependant, il existe un écart important en matière de compétences, la demande de professionnels de l'IA/ML dépassant largement l'offre. Pour y remédier, Emeritus et IITM Pravartak ont lancé un programme de certificat avancé en IA et ML appliquées, dispensé par des professeurs renommés de l'IIT et des experts de l'industrie, offrant une formation rigoureuse et une expérience pratique pour préparer les professionnels à ce domaine en plein essor.\""
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get content\n",
    "summary[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbee76-9d69-4ee2-91ed-8be6ae20d879",
   "metadata": {},
   "source": [
    "### StuffDocumentChain Text Summarization\n",
    "\n",
    "combines all documents into one prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "ca860039-5842-4d16-bcae-fffa220b6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5d3eca33-e94b-42c5-93a1-a3af45a4bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "# path of  pdf file.\n",
    "pdfreader = PdfReader('pdf-sample.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "818c6433-3cc0-420a-bd83-d06d1ac48e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text from pdf file\n",
    "text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6b2a71c2-7fe6-42f2-a55a-d0df146e90c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Adobe Acrobat PDF Files\\nAdobe® Portable Document Format (PDF) is a universal file format that preserves all\\nof the fonts, formatting, colours and graphics of any source document, regardless ofthe application and platform used to create it.\\nAdobe PDF is an ideal format for electronic document distribution as it overcomes the\\nproblems commonly encountered with electronic file sharing.\\n• Anyone, anywhere  can open a PDF file. All you need is the free Adobe Acrobat\\nReader. Recipients of other file formats sometimes can't open files because they\\ndon't have the applications used to create the documents.\\n• PDF files always print correctly  on any printing device.\\n• PDF files always display exactly  as created, regardless of fonts, software, and\\noperating systems. Fonts, and graphics are not lost due to platform, software, and\\nversion incompatibilities.\\n• The free Acrobat Reader is easy to download and can be freely distributed by\\nanyone.\\n• Compact PDF files are smaller than their source files and download a\\npage at a time for fast display on the Web.\""
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "5d0be4ce-a627-4ddf-9ea6-4e73d7a0b580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content=\"Adobe Acrobat PDF Files\\nAdobe® Portable Document Format (PDF) is a universal file format that preserves all\\nof the fonts, formatting, colours and graphics of any source document, regardless ofthe application and platform used to create it.\\nAdobe PDF is an ideal format for electronic document distribution as it overcomes the\\nproblems commonly encountered with electronic file sharing.\\n• Anyone, anywhere  can open a PDF file. All you need is the free Adobe Acrobat\\nReader. Recipients of other file formats sometimes can't open files because they\\ndon't have the applications used to create the documents.\\n• PDF files always print correctly  on any printing device.\\n• PDF files always display exactly  as created, regardless of fonts, software, and\\noperating systems. Fonts, and graphics are not lost due to platform, software, and\\nversion incompatibilities.\\n• The free Acrobat Reader is easy to download and can be freely distributed by\\nanyone.\\n• Compact PDF files are smaller than their source files and download a\\npage at a time for fast display on the Web.\")]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "# Load document\n",
    "docs = [Document(page_content=text)]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "82739478-1e1e-4665-be1e-58b66889ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7, top_p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b4a59640-00d6-4c49-a899-9440408696fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check total tokens\n",
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9cdc2214-1b19-48a6-a49f-f2b681e135b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define template and Instantiate prompt\n",
    "template = \"Write a concise summary of the following: `{text}`\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['text'],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "9804bcfa-c96d-47a7-92c2-7ba6247e13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate chain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='stuff',\n",
    "    prompt=prompt,\n",
    "    verbose=False\n",
    ")\n",
    "summary = chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5defdf99-6eaf-4629-b022-6e99dd94554e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(metadata={}, page_content=\"Adobe Acrobat PDF Files\\nAdobe® Portable Document Format (PDF) is a universal file format that preserves all\\nof the fonts, formatting, colours and graphics of any source document, regardless ofthe application and platform used to create it.\\nAdobe PDF is an ideal format for electronic document distribution as it overcomes the\\nproblems commonly encountered with electronic file sharing.\\n• Anyone, anywhere  can open a PDF file. All you need is the free Adobe Acrobat\\nReader. Recipients of other file formats sometimes can't open files because they\\ndon't have the applications used to create the documents.\\n• PDF files always print correctly  on any printing device.\\n• PDF files always display exactly  as created, regardless of fonts, software, and\\noperating systems. Fonts, and graphics are not lost due to platform, software, and\\nversion incompatibilities.\\n• The free Acrobat Reader is easy to download and can be freely distributed by\\nanyone.\\n• Compact PDF files are smaller than their source files and download a\\npage at a time for fast display on the Web.\")],\n",
       " 'output_text': \"Adobe PDF is a universal file format that preserves a document's original formatting across different platforms and software.  It allows anyone with the free Acrobat Reader to view and print files exactly as intended, regardless of the original creation software.  PDFs are also compact for efficient web sharing.\"}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model response\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "811a67e8-1ca1-4f72-9e6d-c0c6988f22c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Adobe PDF is a universal file format that preserves a document's original formatting across different platforms and software.  It allows anyone with the free Acrobat Reader to view and print files exactly as intended, regardless of the original creation software.  PDFs are also compact for efficient web sharing.\""
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary\n",
    "summary[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91058c9d-9133-4a23-a7dd-08d9e5ed44a5",
   "metadata": {},
   "source": [
    "### Map Reduce Large Documents Text Summarization\n",
    "\n",
    "processes & summarize each document individually, and then aggregates the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6e2b5885-e470-4e43-a584-77df5a39f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "# provide the path of  pdf file.\n",
    "pdfreader = PdfReader('gemini.pdf')\n",
    "# read text from pdf file\n",
    "text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "301a5d83-4553-4c85-8533-609b2282ed4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gemini (language model)\\nGemini (formerly known as Bard) is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, \\nGemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI\\'s GPT-4. It powers the chatbot of the same name. \\nHistory\\nDevelopment\\nGoogle announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor \\nto PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages.[1][2] Unlike other LLMs, Gemini was said to be unique in \\nthat it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code.\\n[3] It had been developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month.[4] In an interview with Wired, \\nDeepMind CEO Demis Hassabis touted Gemini\\'s advanced capabilities, which he believed would allow the algorithm to trump OpenAI\\'s ChatGPT, which runs on GPT-4 and whose growing popularity had \\nbeen aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind\\'s AlphaGo program, which gained worldwide attention in 2016 when it defeated Go \\nchampion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.[5]\\nIn August 2023, The Information published a report outlining Google\\'s roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to \\nsurpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images \\nand be adapted for a wider range of use cases.[6] Like Bard,[7] Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini, along with hundreds of other \\nengineers from Google Brain and DeepMind;[6][8] he was later credited as a \"core contributor\" to Gemini.[9] Because Gemini was being trained on transcripts of YouTube videos, lawyers were brought\\nin to filter out any potentially copyrighted materials.[6]\\nWith news of Gemini\\'s impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini.[10] The Information reported in September that several \\ncompanies had been granted early access to \"an early version\" of the LLM, which Google intended to make available to clients through Google Cloud\\'s Vertex AI service. The publication also stated \\nthat Google was arming Gemini to compete with both GPT-4 and Microsoft\\'s GitHub Copilot.[11][12]\\nLaunch\\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference.[13][14] It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, \\ndesigned for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, \\nwhile Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, \\nAds, Chrome, Duet AI on Google Workspace, and AlphaCode 2.[15][14] It was made available only in English.[14][16] Touted as Google\\'s \"largest and most capable AI model\" and designed to \\nemulate human behavior,[17][14][18] the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\".[13] Gemini was \\ntrained on and powered by Google\\'s Tensor Processing Units (TPUs),[13][16] and the name is in reference to the DeepMind–Google Brain merger as well as NASA\\'s Project Gemini.[19]\\nGemini Ultra was said to have outperformed GPT-4, Anthropic\\'s Claude 2, Inflection AI\\'s Inflection-2, Meta\\'s LLaMA 2, and xAI\\'s Grok 1 on a variety of industry benchmarks,[20][13] while Gemini\\nPro was said to have outperformed GPT-3.5.[3] Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) \\ntest, obtaining a score of 90%.[3][19] Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android \\ndevelopers as well.[21][22][23] Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\".[24] In accordance with an \\nexecutive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, \\nthe company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.[3]\\nUpdates\\nGoogle partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024.[25][26] The following month, Bard and Duet AI were unified \\nunder the Gemini brand,[27][28] with \"Gemini Advanced with Ultra 1.0\" debuting via a new \"AI Premium\" tier of the Google One subscription service.[29] Gemini Pro also received a global launch.[30]\\nIn February, 2024, Google launched \"Gemini 1.5\" in a limited capacity, positioned as a more powerful and capable model than 1.0 Ultra.[31][32][33] This \"step change\" was achieved through various\\ntechnical advancements, including a new architecture, a mixture-of-experts approach, and a larger one-million-token context window, which equates to roughly an hour of silent video, 11 hours of audio, \\n30,000 lines of code, or 700,000 words.[34] The same month, Google debuted Gemma, a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes, \\nwith a neural network with two and seven billion parameters, respectively. Multiple publications viewed this as a response to Meta and others open-sourcing their AI models, and a stark reversal from \\nGoogle\\'s longstanding practice of keeping its AI proprietary.[35][36][37] Google announced an additional model, Gemini 1.5 Flash, on May 14th at the 2024 I/O keynote.[38]\\nGemma 2 was released on June 27, 2024.[39]\\nTwo updated Gemini models, Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002, were released on September 24, 2024.[40]\\nOn December 11, 2024, Google announced Gemini 2.0 Flash Experimental,[41] a significant update to its Gemini AI model. This iteration boasts improved speed and performance over its predecessor, \\nGemini 1.5 Flash. Key features include a Multimodal Live API for real-time audio and video interactions, enhanced spatial understanding, native image and controllable text-to-speech generation \\n(with watermarking), and integrated tool use, including Google Search.[42] It also introduces improved agentic capabilities, a new Google Gen AI SDK,[43] and \"Jules,\" an experimental AI coding \\nagent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all \\nusers as \"Gemini 2.0 Flash experimental\". \\nOn January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 05, \\n2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model\\'s thinking process when responding to prompts.[44]\\nTechnical specifications\\nThe first generation of Gemini (\"Gemini 1\") has three models, with the same architecture. They are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. \\nThey have a context length of 32,768 tokens, with multi-query attention. Two versions of Gemini Nano, Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), are distilled from larger \\nGemini models, designed for use by edge devices such as smartphones. As Gemini is multimodal, each context window can contain multiple forms of input. The different modes can be interleaved \\nand do not have to be presented in a fixed order, allowing for a multimodal conversation. For example, the user might open the conversation with a mix of text, picture, video, and audio, presented in \\nany order, and Gemini might reply with the same free ordering. Input images may be of different resolutions, while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then \\nconverted into a sequence of tokens by the Universal Speech Model. Gemini\\'s dataset is multimodal and multilingual, consisting of \"web documents, books, and code, and includ[ing] image, audio, \\nand video data\".[45]\\nThe second generation of Gemini (\"Gemini 1.5\") has two models. Gemini 1.5 Pro is a multimodal sparse mixture-of-experts, with a context length in the millions, while Gemini 1.5 Flash is distilled from \\nGemini 1.5 Pro, with a context length above 2 million.[46]\\nGemma 2 27B is trained on web documents, code, science articles. Gemma 2 9B was distilled from 27B. Gemma 2 2B was distilled from a 7B model that remained unreleased.[47]\\n'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "bff6edda-0fbf-45f2-9eab-f3a38d03efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# Instantiate model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7, top_p=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d517aeeb-7db2-4aee-ae9f-09d17519fef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2362"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get total tokens\n",
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "cdd36edf-0c0d-4d15-9887-06361dfca1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Splittting the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
    "chunks = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0216e247-977d-4e1c-9692-b64be12b9b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "39c20dcd-707c-4188-9518-ddb49d57bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "# Instantiate chain\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=False\n",
    ")\n",
    "summary = chain.invoke(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3394b8f9-c474-4661-8334-51f6c8b0a369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(metadata={}, page_content='Gemini (language model)\\nGemini (formerly known as Bard) is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, \\nGemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI\\'s GPT-4. It powers the chatbot of the same name. \\nHistory\\nDevelopment\\nGoogle announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor \\nto PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages.[1][2] Unlike other LLMs, Gemini was said to be unique in \\nthat it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code.\\n[3] It had been developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month.[4] In an interview with Wired, \\nDeepMind CEO Demis Hassabis touted Gemini\\'s advanced capabilities, which he believed would allow the algorithm to trump OpenAI\\'s ChatGPT, which runs on GPT-4 and whose growing popularity had \\nbeen aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind\\'s AlphaGo program, which gained worldwide attention in 2016 when it defeated Go \\nchampion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.[5]\\nIn August 2023, The Information published a report outlining Google\\'s roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to \\nsurpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images \\nand be adapted for a wider range of use cases.[6] Like Bard,[7] Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini, along with hundreds of other \\nengineers from Google Brain and DeepMind;[6][8] he was later credited as a \"core contributor\" to Gemini.[9] Because Gemini was being trained on transcripts of YouTube videos, lawyers were brought\\nin to filter out any potentially copyrighted materials.[6]\\nWith news of Gemini\\'s impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini.[10] The Information reported in September that several \\ncompanies had been granted early access to \"an early version\" of the LLM, which Google intended to make available to clients through Google Cloud\\'s Vertex AI service. The publication also stated \\nthat Google was arming Gemini to compete with both GPT-4 and Microsoft\\'s GitHub Copilot.[11][12]\\nLaunch\\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference.[13][14] It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, \\ndesigned for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, \\nwhile Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, \\nAds, Chrome, Duet AI on Google Workspace, and AlphaCode 2.[15][14] It was made available only in English.[14][16] Touted as Google\\'s \"largest and most capable AI model\" and designed to \\nemulate human behavior,[17][14][18] the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\".[13] Gemini was \\ntrained on and powered by Google\\'s Tensor Processing Units (TPUs),[13][16] and the name is in reference to the DeepMind–Google Brain merger as well as NASA\\'s Project Gemini.[19]\\nGemini Ultra was said to have outperformed GPT-4, Anthropic\\'s Claude 2, Inflection AI\\'s Inflection-2, Meta\\'s LLaMA 2, and xAI\\'s Grok 1 on a variety of industry benchmarks,[20][13] while Gemini\\nPro was said to have outperformed GPT-3.5.[3] Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) \\ntest, obtaining a score of 90%.[3][19] Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android \\ndevelopers as well.[21][22][23] Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\".[24] In accordance with an \\nexecutive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, \\nthe company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.[3]\\nUpdates\\nGoogle partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024.[25][26] The following month, Bard and Duet AI were unified \\nunder the Gemini brand,[27][28] with \"Gemini Advanced with Ultra 1.0\" debuting via a new \"AI Premium\" tier of the Google One subscription service.[29] Gemini Pro also received a global launch.[30]\\nIn February, 2024, Google launched \"Gemini 1.5\" in a limited capacity, positioned as a more powerful and capable model than 1.0 Ultra.[31][32][33] This \"step change\" was achieved through various\\ntechnical advancements, including a new architecture, a mixture-of-experts approach, and a larger one-million-token context window, which equates to roughly an hour of silent video, 11 hours of audio, \\n30,000 lines of code, or 700,000 words.[34] The same month, Google debuted Gemma, a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes, \\nwith a neural network with two and seven billion parameters, respectively. Multiple publications viewed this as a response to Meta and others open-sourcing their AI models, and a stark reversal from \\nGoogle\\'s longstanding practice of keeping its AI proprietary.[35][36][37] Google announced an additional model, Gemini 1.5 Flash, on May 14th at the 2024 I/O keynote.[38]\\nGemma 2 was released on June 27, 2024.[39]\\nTwo updated Gemini models, Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002, were released on September 24, 2024.[40]\\nOn December 11, 2024, Google announced Gemini 2.0 Flash Experimental,[41] a significant update to its Gemini AI model. This iteration boasts improved speed and performance over its predecessor, \\nGemini 1.5 Flash. Key features include a Multimodal Live API for real-time audio and video interactions, enhanced spatial understanding, native image and controllable text-to-speech generation \\n(with watermarking), and integrated tool use, including Google Search.[42] It also introduces improved agentic capabilities, a new Google Gen AI SDK,[43] and \"Jules,\" an experimental AI coding \\nagent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all \\nusers as \"Gemini 2.0 Flash experimental\". \\nOn January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 05, \\n2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model\\'s thinking process when responding to prompts.[44]\\nTechnical specifications\\nThe first generation of Gemini (\"Gemini 1\") has three models, with the same architecture. They are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. \\nThey have a context length of 32,768 tokens, with multi-query attention. Two versions of Gemini Nano, Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), are distilled from larger \\nGemini models, designed for use by edge devices such as smartphones. As Gemini is multimodal, each context window can contain multiple forms of input. The different modes can be interleaved \\nand do not have to be presented in a fixed order, allowing for a multimodal conversation. For example, the user might open the conversation with a mix of text, picture, video, and audio, presented in \\nany order, and Gemini might reply with the same free ordering. Input images may be of different resolutions, while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then \\nconverted into a sequence of tokens by the Universal Speech Model. Gemini\\'s dataset is multimodal and multilingual, consisting of \"web documents, books, and code, and includ[ing] image, audio, \\nand video data\".[45]\\nThe second generation of Gemini (\"Gemini 1.5\") has two models. Gemini 1.5 Pro is a multimodal sparse mixture-of-experts, with a context length in the millions, while Gemini 1.5 Flash is distilled from \\nGemini 1.5 Pro, with a context length above 2 million.[46]\\nGemma 2 27B is trained on web documents, code, science articles. Gemma 2 9B was distilled from 27B. Gemma 2 2B was distilled from a 7B model that remained unreleased.[47]')],\n",
       " 'output_text': \"Google DeepMind's Gemini, a multimodal LLM family succeeding LaMDA and PaLM 2, launched in late 2023 with various versions (Ultra, Pro, Nano, Flash).  It rivals GPT-4, powers Google products, and excels in multimodal processing, outperforming competitors on key benchmarks.  Development continues with improved versions and open-source Gemma models.\"}"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "95861c6d-708f-483d-b25c-1f9e48902fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Google DeepMind's Gemini, a multimodal LLM family succeeding LaMDA and PaLM 2, launched in late 2023 with various versions (Ultra, Pro, Nano, Flash).  It rivals GPT-4, powers Google products, and excels in multimodal processing, outperforming competitors on key benchmarks.  Development continues with improved versions and open-source Gemma models.\""
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[\"output_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3f840-466e-4ab6-8d9f-69b53881f9d8",
   "metadata": {},
   "source": [
    "### Map Reduce With Custom Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "fc2d3bf6-3389-47e2-a23b-02fe4f1404a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunk template and Instantiate prompt\n",
    "template=\"\"\"\n",
    "Please summarize the following:\\\\n`{text}'\n",
    "Summary:\n",
    "\"\"\"\n",
    "prompt_template=PromptTemplate(input_variables=['text'],\n",
    "                                    template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "98e917bd-43ca-4eeb-90b8-e14c5446f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final template and Instantiate prompt\n",
    "final_template=\"\"\"\n",
    "Please provide a final summary of the text with these important points.\n",
    "Add a Title,\n",
    "Start the summary with an introduction \n",
    "Provide the summary in number points for the text.\n",
    "Summary: `{text}`\n",
    "\"\"\"\n",
    "final_prompt_template=PromptTemplate(input_variables=['text'],\n",
    "                                             template=final_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "d646fba5-2bf0-420a-a8ed-ee6232992706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "# Instantiate chain\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=prompt_template,\n",
    "    combine_prompt=final_prompt_template,\n",
    "    verbose=False\n",
    ")\n",
    "summary = chain.invoke(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "d4f24351-d25f-4179-9f70-ec3eb29c475f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(metadata={}, page_content='Gemini (language model)\\nGemini (formerly known as Bard) is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, \\nGemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI\\'s GPT-4. It powers the chatbot of the same name. \\nHistory\\nDevelopment\\nGoogle announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor \\nto PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages.[1][2] Unlike other LLMs, Gemini was said to be unique in \\nthat it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code.\\n[3] It had been developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month.[4] In an interview with Wired, \\nDeepMind CEO Demis Hassabis touted Gemini\\'s advanced capabilities, which he believed would allow the algorithm to trump OpenAI\\'s ChatGPT, which runs on GPT-4 and whose growing popularity had \\nbeen aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind\\'s AlphaGo program, which gained worldwide attention in 2016 when it defeated Go \\nchampion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.[5]\\nIn August 2023, The Information published a report outlining Google\\'s roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to \\nsurpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images \\nand be adapted for a wider range of use cases.[6] Like Bard,[7] Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini, along with hundreds of other \\nengineers from Google Brain and DeepMind;[6][8] he was later credited as a \"core contributor\" to Gemini.[9] Because Gemini was being trained on transcripts of YouTube videos, lawyers were brought\\nin to filter out any potentially copyrighted materials.[6]\\nWith news of Gemini\\'s impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini.[10] The Information reported in September that several \\ncompanies had been granted early access to \"an early version\" of the LLM, which Google intended to make available to clients through Google Cloud\\'s Vertex AI service. The publication also stated \\nthat Google was arming Gemini to compete with both GPT-4 and Microsoft\\'s GitHub Copilot.[11][12]\\nLaunch\\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference.[13][14] It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, \\ndesigned for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, \\nwhile Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, \\nAds, Chrome, Duet AI on Google Workspace, and AlphaCode 2.[15][14] It was made available only in English.[14][16] Touted as Google\\'s \"largest and most capable AI model\" and designed to \\nemulate human behavior,[17][14][18] the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\".[13] Gemini was \\ntrained on and powered by Google\\'s Tensor Processing Units (TPUs),[13][16] and the name is in reference to the DeepMind–Google Brain merger as well as NASA\\'s Project Gemini.[19]\\nGemini Ultra was said to have outperformed GPT-4, Anthropic\\'s Claude 2, Inflection AI\\'s Inflection-2, Meta\\'s LLaMA 2, and xAI\\'s Grok 1 on a variety of industry benchmarks,[20][13] while Gemini\\nPro was said to have outperformed GPT-3.5.[3] Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) \\ntest, obtaining a score of 90%.[3][19] Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android \\ndevelopers as well.[21][22][23] Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\".[24] In accordance with an \\nexecutive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, \\nthe company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.[3]\\nUpdates\\nGoogle partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024.[25][26] The following month, Bard and Duet AI were unified \\nunder the Gemini brand,[27][28] with \"Gemini Advanced with Ultra 1.0\" debuting via a new \"AI Premium\" tier of the Google One subscription service.[29] Gemini Pro also received a global launch.[30]\\nIn February, 2024, Google launched \"Gemini 1.5\" in a limited capacity, positioned as a more powerful and capable model than 1.0 Ultra.[31][32][33] This \"step change\" was achieved through various\\ntechnical advancements, including a new architecture, a mixture-of-experts approach, and a larger one-million-token context window, which equates to roughly an hour of silent video, 11 hours of audio, \\n30,000 lines of code, or 700,000 words.[34] The same month, Google debuted Gemma, a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes, \\nwith a neural network with two and seven billion parameters, respectively. Multiple publications viewed this as a response to Meta and others open-sourcing their AI models, and a stark reversal from \\nGoogle\\'s longstanding practice of keeping its AI proprietary.[35][36][37] Google announced an additional model, Gemini 1.5 Flash, on May 14th at the 2024 I/O keynote.[38]\\nGemma 2 was released on June 27, 2024.[39]\\nTwo updated Gemini models, Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002, were released on September 24, 2024.[40]\\nOn December 11, 2024, Google announced Gemini 2.0 Flash Experimental,[41] a significant update to its Gemini AI model. This iteration boasts improved speed and performance over its predecessor, \\nGemini 1.5 Flash. Key features include a Multimodal Live API for real-time audio and video interactions, enhanced spatial understanding, native image and controllable text-to-speech generation \\n(with watermarking), and integrated tool use, including Google Search.[42] It also introduces improved agentic capabilities, a new Google Gen AI SDK,[43] and \"Jules,\" an experimental AI coding \\nagent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all \\nusers as \"Gemini 2.0 Flash experimental\". \\nOn January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 05, \\n2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model\\'s thinking process when responding to prompts.[44]\\nTechnical specifications\\nThe first generation of Gemini (\"Gemini 1\") has three models, with the same architecture. They are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. \\nThey have a context length of 32,768 tokens, with multi-query attention. Two versions of Gemini Nano, Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), are distilled from larger \\nGemini models, designed for use by edge devices such as smartphones. As Gemini is multimodal, each context window can contain multiple forms of input. The different modes can be interleaved \\nand do not have to be presented in a fixed order, allowing for a multimodal conversation. For example, the user might open the conversation with a mix of text, picture, video, and audio, presented in \\nany order, and Gemini might reply with the same free ordering. Input images may be of different resolutions, while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then \\nconverted into a sequence of tokens by the Universal Speech Model. Gemini\\'s dataset is multimodal and multilingual, consisting of \"web documents, books, and code, and includ[ing] image, audio, \\nand video data\".[45]\\nThe second generation of Gemini (\"Gemini 1.5\") has two models. Gemini 1.5 Pro is a multimodal sparse mixture-of-experts, with a context length in the millions, while Gemini 1.5 Flash is distilled from \\nGemini 1.5 Pro, with a context length above 2 million.[46]\\nGemma 2 27B is trained on web documents, code, science articles. Gemma 2 9B was distilled from 27B. Gemma 2 2B was distilled from a 7B model that remained unreleased.[47]')],\n",
       " 'output_text': \"Title: Gemini: Google's Multimodal LLM Powerhouse\\n\\nIntroduction: Gemini is Google's answer to the rapidly evolving landscape of large language models (LLMs). Developed by Google DeepMind, this family of multimodal models aims to surpass its predecessors and compete with leading models like OpenAI's GPT-4.  Since its launch in December 2023, Gemini has undergone significant development, showcasing impressive capabilities and continuous improvements.\\n\\nSummary:\\n\\n1. **Multimodal Mastery:** Gemini's core strength lies in its multimodal processing, handling text, images, audio, video, and code. This allows for richer and more versatile interactions compared to text-only models.\\n\\n2. **Scalable Sizes:**  Gemini is available in various sizes, catering to different needs and resource constraints. These include Ultra (the most powerful), Pro, Flash, and Nano (designed for on-device applications).\\n\\n3. **Collaborative Development:** Gemini's development benefited from the combined expertise of DeepMind and Google Brain, leveraging knowledge from projects like AlphaGo.\\n\\n4. **Extensive Training Data and Safety:**  Trained on a massive dataset, including YouTube transcripts, Gemini's development included legal review to address copyright concerns.  Google prioritized rigorous safety testing before release.\\n\\n5. **Benchmark Performance:**  Gemini Ultra demonstrated superior performance compared to competitors on several benchmarks, notably the Massive Multitask Language Understanding (MMLU) test.\\n\\n6. **Continuous Updates and Integrations:**  Since launch, Gemini has seen continuous updates, including integration with Samsung's Galaxy S24, the unification of Bard and Duet AI under the Gemini brand, and the release of more powerful 1.5 and 2.0 versions with expanded context windows and enhanced multimodal capabilities.\\n\\n7. **Open-Source Offering:** Google also released Gemma, an open-source, lightweight version of Gemini, fostering community development and accessibility.\\n\\n8. **Evolving Capabilities:** Gemini continues to evolve with improved performance, new features like real-time audio/video interaction and tool use, and broader availability through platforms such as Google Colab and GitHub.\"}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model response\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e5194a87-f119-4a9e-b8ee-bc61b09f5e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Gemini: Google's Multimodal LLM Powerhouse\n",
      "\n",
      "Introduction: Gemini is Google's answer to the rapidly evolving landscape of large language models (LLMs). Developed by Google DeepMind, this family of multimodal models aims to surpass its predecessors and compete with leading models like OpenAI's GPT-4.  Since its launch in December 2023, Gemini has undergone significant development, showcasing impressive capabilities and continuous improvements.\n",
      "\n",
      "Summary:\n",
      "\n",
      "1. **Multimodal Mastery:** Gemini's core strength lies in its multimodal processing, handling text, images, audio, video, and code. This allows for richer and more versatile interactions compared to text-only models.\n",
      "\n",
      "2. **Scalable Sizes:**  Gemini is available in various sizes, catering to different needs and resource constraints. These include Ultra (the most powerful), Pro, Flash, and Nano (designed for on-device applications).\n",
      "\n",
      "3. **Collaborative Development:** Gemini's development benefited from the combined expertise of DeepMind and Google Brain, leveraging knowledge from projects like AlphaGo.\n",
      "\n",
      "4. **Extensive Training Data and Safety:**  Trained on a massive dataset, including YouTube transcripts, Gemini's development included legal review to address copyright concerns.  Google prioritized rigorous safety testing before release.\n",
      "\n",
      "5. **Benchmark Performance:**  Gemini Ultra demonstrated superior performance compared to competitors on several benchmarks, notably the Massive Multitask Language Understanding (MMLU) test.\n",
      "\n",
      "6. **Continuous Updates and Integrations:**  Since launch, Gemini has seen continuous updates, including integration with Samsung's Galaxy S24, the unification of Bard and Duet AI under the Gemini brand, and the release of more powerful 1.5 and 2.0 versions with expanded context windows and enhanced multimodal capabilities.\n",
      "\n",
      "7. **Open-Source Offering:** Google also released Gemma, an open-source, lightweight version of Gemini, fostering community development and accessibility.\n",
      "\n",
      "8. **Evolving Capabilities:** Gemini continues to evolve with improved performance, new features like real-time audio/video interaction and tool use, and broader availability through platforms such as Google Colab and GitHub.\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(summary[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457d90d-e963-45c1-8639-9d30d63530d2",
   "metadata": {},
   "source": [
    "### RefineChain Text Summarization\n",
    "\n",
    "iteratively improves an answer by feeding back previous results to the model with new context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "49d6cf47-5cdf-42e3-a5b0-2774b85be28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Gemini (language model)\n",
      "Gemini (formerly known as Bard) is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, \n",
      "Gemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI's GPT-4. It powers the chatbot of the same name. \n",
      "History\n",
      "Development\n",
      "Google announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor \n",
      "to PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages.[1][2] Unlike other LLMs, Gemini was said to be unique in \n",
      "that it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code.\n",
      "[3] It had been developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month.[4] In an interview with Wired, \n",
      "DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities, which he believed would allow the algorithm to trump OpenAI's ChatGPT, which runs on GPT-4 and whose growing popularity had \n",
      "been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind's AlphaGo program, which gained worldwide attention in 2016 when it defeated Go \n",
      "champion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.[5]\n",
      "In August 2023, The Information published a report outlining Google's roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to \n",
      "surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images \n",
      "and be adapted for a wider range of use cases.[6] Like Bard,[7] Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini, along with hundreds of other \n",
      "engineers from Google Brain and DeepMind;[6][8] he was later credited as a \"core contributor\" to Gemini.[9] Because Gemini was being trained on transcripts of YouTube videos, lawyers were brought\n",
      "in to filter out any potentially copyrighted materials.[6]\n",
      "With news of Gemini's impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini.[10] The Information reported in September that several \n",
      "companies had been granted early access to \"an early version\" of the LLM, which Google intended to make available to clients through Google Cloud's Vertex AI service. The publication also stated \n",
      "that Google was arming Gemini to compete with both GPT-4 and Microsoft's GitHub Copilot.[11][12]\n",
      "Launch\n",
      "On December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference.[13][14] It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, \n",
      "designed for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, \n",
      "while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, \n",
      "Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.[15][14] It was made available only in English.[14][16] Touted as Google's \"largest and most capable AI model\" and designed to \n",
      "emulate human behavior,[17][14][18] the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\".[13] Gemini was \n",
      "trained on and powered by Google's Tensor Processing Units (TPUs),[13][16] and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.[19]\n",
      "Gemini Ultra was said to have outperformed GPT-4, Anthropic's Claude 2, Inflection AI's Inflection-2, Meta's LLaMA 2, and xAI's Grok 1 on a variety of industry benchmarks,[20][13] while Gemini\n",
      "Pro was said to have outperformed GPT-3.5.[3] Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) \n",
      "test, obtaining a score of 90%.[3][19] Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android \n",
      "developers as well.[21][22][23] Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\".[24] In accordance with an \n",
      "executive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, \n",
      "the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.[3]\n",
      "Updates\n",
      "Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024.[25][26] The following month, Bard and Duet AI were unified \n",
      "under the Gemini brand,[27][28] with \"Gemini Advanced with Ultra 1.0\" debuting via a new \"AI Premium\" tier of the Google One subscription service.[29] Gemini Pro also received a global launch.[30]\n",
      "In February, 2024, Google launched \"Gemini 1.5\" in a limited capacity, positioned as a more powerful and capable model than 1.0 Ultra.[31][32][33] This \"step change\" was achieved through various\n",
      "technical advancements, including a new architecture, a mixture-of-experts approach, and a larger one-million-token context window, which equates to roughly an hour of silent video, 11 hours of audio, \n",
      "30,000 lines of code, or 700,000 words.[34] The same month, Google debuted Gemma, a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes, \n",
      "with a neural network with two and seven billion parameters, respectively. Multiple publications viewed this as a response to Meta and others open-sourcing their AI models, and a stark reversal from \n",
      "Google's longstanding practice of keeping its AI proprietary.[35][36][37] Google announced an additional model, Gemini 1.5 Flash, on May 14th at the 2024 I/O keynote.[38]\n",
      "Gemma 2 was released on June 27, 2024.[39]\n",
      "Two updated Gemini models, Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002, were released on September 24, 2024.[40]\n",
      "On December 11, 2024, Google announced Gemini 2.0 Flash Experimental,[41] a significant update to its Gemini AI model. This iteration boasts improved speed and performance over its predecessor, \n",
      "Gemini 1.5 Flash. Key features include a Multimodal Live API for real-time audio and video interactions, enhanced spatial understanding, native image and controllable text-to-speech generation \n",
      "(with watermarking), and integrated tool use, including Google Search.[42] It also introduces improved agentic capabilities, a new Google Gen AI SDK,[43] and \"Jules,\" an experimental AI coding \n",
      "agent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all \n",
      "users as \"Gemini 2.0 Flash experimental\". \n",
      "On January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 05, \n",
      "2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model's thinking process when responding to prompts.[44]\n",
      "Technical specifications\n",
      "The first generation of Gemini (\"Gemini 1\") has three models, with the same architecture. They are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. \n",
      "They have a context length of 32,768 tokens, with multi-query attention. Two versions of Gemini Nano, Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), are distilled from larger \n",
      "Gemini models, designed for use by edge devices such as smartphones. As Gemini is multimodal, each context window can contain multiple forms of input. The different modes can be interleaved \n",
      "and do not have to be presented in a fixed order, allowing for a multimodal conversation. For example, the user might open the conversation with a mix of text, picture, video, and audio, presented in \n",
      "any order, and Gemini might reply with the same free ordering. Input images may be of different resolutions, while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then \n",
      "converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual, consisting of \"web documents, books, and code, and includ[ing] image, audio, \n",
      "and video data\".[45]\n",
      "The second generation of Gemini (\"Gemini 1.5\") has two models. Gemini 1.5 Pro is a multimodal sparse mixture-of-experts, with a context length in the millions, while Gemini 1.5 Flash is distilled from \n",
      "Gemini 1.5 Pro, with a context length above 2 million.[46]\n",
      "Gemma 2 27B is trained on web documents, code, science articles. Gemma 2 9B was distilled from 27B. Gemma 2 2B was distilled from a 7B model that remained unreleased.[47]\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "# Instantiate chain\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    verbose=True\n",
    ")\n",
    "summary = chain.invoke(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d3534384-3b45-4f5b-9fae-5805d9c67185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(metadata={}, page_content='Gemini (language model)\\nGemini (formerly known as Bard) is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, \\nGemini Pro, Gemini Flash, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI\\'s GPT-4. It powers the chatbot of the same name. \\nHistory\\nDevelopment\\nGoogle announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor \\nto PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages.[1][2] Unlike other LLMs, Gemini was said to be unique in \\nthat it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code.\\n[3] It had been developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month.[4] In an interview with Wired, \\nDeepMind CEO Demis Hassabis touted Gemini\\'s advanced capabilities, which he believed would allow the algorithm to trump OpenAI\\'s ChatGPT, which runs on GPT-4 and whose growing popularity had \\nbeen aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind\\'s AlphaGo program, which gained worldwide attention in 2016 when it defeated Go \\nchampion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.[5]\\nIn August 2023, The Information published a report outlining Google\\'s roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to \\nsurpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation, allowing it to create contextual images \\nand be adapted for a wider range of use cases.[6] Like Bard,[7] Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini, along with hundreds of other \\nengineers from Google Brain and DeepMind;[6][8] he was later credited as a \"core contributor\" to Gemini.[9] Because Gemini was being trained on transcripts of YouTube videos, lawyers were brought\\nin to filter out any potentially copyrighted materials.[6]\\nWith news of Gemini\\'s impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini.[10] The Information reported in September that several \\ncompanies had been granted early access to \"an early version\" of the LLM, which Google intended to make available to clients through Google Cloud\\'s Vertex AI service. The publication also stated \\nthat Google was arming Gemini to compete with both GPT-4 and Microsoft\\'s GitHub Copilot.[11][12]\\nLaunch\\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference.[13][14] It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, \\ndesigned for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, \\nwhile Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, \\nAds, Chrome, Duet AI on Google Workspace, and AlphaCode 2.[15][14] It was made available only in English.[14][16] Touted as Google\\'s \"largest and most capable AI model\" and designed to \\nemulate human behavior,[17][14][18] the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\".[13] Gemini was \\ntrained on and powered by Google\\'s Tensor Processing Units (TPUs),[13][16] and the name is in reference to the DeepMind–Google Brain merger as well as NASA\\'s Project Gemini.[19]\\nGemini Ultra was said to have outperformed GPT-4, Anthropic\\'s Claude 2, Inflection AI\\'s Inflection-2, Meta\\'s LLaMA 2, and xAI\\'s Grok 1 on a variety of industry benchmarks,[20][13] while Gemini\\nPro was said to have outperformed GPT-3.5.[3] Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) \\ntest, obtaining a score of 90%.[3][19] Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android \\ndevelopers as well.[21][22][23] Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\".[24] In accordance with an \\nexecutive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, \\nthe company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.[3]\\nUpdates\\nGoogle partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024.[25][26] The following month, Bard and Duet AI were unified \\nunder the Gemini brand,[27][28] with \"Gemini Advanced with Ultra 1.0\" debuting via a new \"AI Premium\" tier of the Google One subscription service.[29] Gemini Pro also received a global launch.[30]\\nIn February, 2024, Google launched \"Gemini 1.5\" in a limited capacity, positioned as a more powerful and capable model than 1.0 Ultra.[31][32][33] This \"step change\" was achieved through various\\ntechnical advancements, including a new architecture, a mixture-of-experts approach, and a larger one-million-token context window, which equates to roughly an hour of silent video, 11 hours of audio, \\n30,000 lines of code, or 700,000 words.[34] The same month, Google debuted Gemma, a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes, \\nwith a neural network with two and seven billion parameters, respectively. Multiple publications viewed this as a response to Meta and others open-sourcing their AI models, and a stark reversal from \\nGoogle\\'s longstanding practice of keeping its AI proprietary.[35][36][37] Google announced an additional model, Gemini 1.5 Flash, on May 14th at the 2024 I/O keynote.[38]\\nGemma 2 was released on June 27, 2024.[39]\\nTwo updated Gemini models, Gemini-1.5-Pro-002 and Gemini-1.5-Flash-002, were released on September 24, 2024.[40]\\nOn December 11, 2024, Google announced Gemini 2.0 Flash Experimental,[41] a significant update to its Gemini AI model. This iteration boasts improved speed and performance over its predecessor, \\nGemini 1.5 Flash. Key features include a Multimodal Live API for real-time audio and video interactions, enhanced spatial understanding, native image and controllable text-to-speech generation \\n(with watermarking), and integrated tool use, including Google Search.[42] It also introduces improved agentic capabilities, a new Google Gen AI SDK,[43] and \"Jules,\" an experimental AI coding \\nagent for GitHub. Additionally, Google Colab is integrating Gemini 2.0 to generate data science notebooks from natural language. Gemini 2.0 was available through the Gemini chat interface for all \\nusers as \"Gemini 2.0 Flash experimental\". \\nOn January 30, 2025, Google released Gemini 2.0 Flash as the new default model, with Gemini 1.5 Flash still available for usage. This was followed by the release of Gemini 2.0 Pro on February 05, \\n2025. Additionally, Google released Gemini 2.0 Flash Thinking Experimental, which details the language model\\'s thinking process when responding to prompts.[44]\\nTechnical specifications\\nThe first generation of Gemini (\"Gemini 1\") has three models, with the same architecture. They are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. \\nThey have a context length of 32,768 tokens, with multi-query attention. Two versions of Gemini Nano, Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), are distilled from larger \\nGemini models, designed for use by edge devices such as smartphones. As Gemini is multimodal, each context window can contain multiple forms of input. The different modes can be interleaved \\nand do not have to be presented in a fixed order, allowing for a multimodal conversation. For example, the user might open the conversation with a mix of text, picture, video, and audio, presented in \\nany order, and Gemini might reply with the same free ordering. Input images may be of different resolutions, while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then \\nconverted into a sequence of tokens by the Universal Speech Model. Gemini\\'s dataset is multimodal and multilingual, consisting of \"web documents, books, and code, and includ[ing] image, audio, \\nand video data\".[45]\\nThe second generation of Gemini (\"Gemini 1.5\") has two models. Gemini 1.5 Pro is a multimodal sparse mixture-of-experts, with a context length in the millions, while Gemini 1.5 Flash is distilled from \\nGemini 1.5 Pro, with a context length above 2 million.[46]\\nGemma 2 27B is trained on web documents, code, science articles. Gemma 2 9B was distilled from 27B. Gemma 2 2B was distilled from a 7B model that remained unreleased.[47]')],\n",
       " 'output_text': \"The Rise of Gemini: Google's Multimodal LLM\\n\\nIntroduction:\\nGemini represents Google's ambitious foray into the realm of multimodal large language models (LLMs), aiming to surpass competitors like OpenAI's GPT-4. This summary chronicles Gemini's development, from its initial announcement to its current iteration, highlighting key milestones and features.\\n\\n1. Initial Promise (May 2023):\\n   - Announced as the successor to PaLM 2, boasting multimodal capabilities (text, images, audio, video, code).\\n   - Jointly developed by the newly merged DeepMind and Google Brain.\\n   - Positioned as a more powerful and versatile LLM.\\n\\n2. Building Anticipation (August-November 2023):\\n   - Targeted a late 2023 launch, emphasizing combined text and image generation.\\n   - Sergey Brin's involvement highlighted the project's importance.\\n   - Training data included YouTube transcripts, necessitating legal review.\\n   - OpenAI responded by accelerating its own multimodal development.\\n   - Early access granted to select companies via Google Cloud.\\n\\n3. Gemini 1.0 Arrives (December 2023):\\n   - Three models launched: Ultra (complex tasks), Pro (wide range), and Nano (on-device).\\n   - Integration into Bard, Pixel 8 Pro, and other Google products planned.\\n   - Benchmark performance surpassed competitors, with Ultra exceeding human experts on the MMLU test.\\n   - Initial release limited to English due to ongoing safety testing.\\n\\n4. Evolution and Expansion (2024-2025):\\n   - Partnership with Samsung for Galaxy S24 integration.\\n   - Bard and Duet AI unified under the Gemini brand.\\n   - Gemini 1.5 released with increased power and context window.\\n   - Gemma, an open-source, lightweight version, launched.\\n   - Gemini 1.5 Flash introduced.\\n   - Gemini 2.0 Flash Experimental released with enhanced multimodal capabilities, tool integration, and improved performance, later becoming the default.\\n   - Gemini 2.0 Pro released.\\n\\n5. Architecture and Focus:\\n   - Based on decoder-only transformers, optimized for TPUs.\\n   - Utilizes a multimodal, multilingual dataset.\\n   - Features a flexible context window handling various data types in any order.\\n   - Different model generations and sizes cater to various needs, from cloud-based to on-device applications.\\n\\n6. Responsible Development:\\n   - Google emphasizes responsible AI development, prioritizing safety testing.\\n   - Collaboration with governments on AI safety is a key focus. \"}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "68beafdf-59b2-4d85-b3a8-456b11630b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rise of Gemini: Google's Multimodal LLM\n",
      "\n",
      "Introduction:\n",
      "Gemini represents Google's ambitious foray into the realm of multimodal large language models (LLMs), aiming to surpass competitors like OpenAI's GPT-4. This summary chronicles Gemini's development, from its initial announcement to its current iteration, highlighting key milestones and features.\n",
      "\n",
      "1. Initial Promise (May 2023):\n",
      "   - Announced as the successor to PaLM 2, boasting multimodal capabilities (text, images, audio, video, code).\n",
      "   - Jointly developed by the newly merged DeepMind and Google Brain.\n",
      "   - Positioned as a more powerful and versatile LLM.\n",
      "\n",
      "2. Building Anticipation (August-November 2023):\n",
      "   - Targeted a late 2023 launch, emphasizing combined text and image generation.\n",
      "   - Sergey Brin's involvement highlighted the project's importance.\n",
      "   - Training data included YouTube transcripts, necessitating legal review.\n",
      "   - OpenAI responded by accelerating its own multimodal development.\n",
      "   - Early access granted to select companies via Google Cloud.\n",
      "\n",
      "3. Gemini 1.0 Arrives (December 2023):\n",
      "   - Three models launched: Ultra (complex tasks), Pro (wide range), and Nano (on-device).\n",
      "   - Integration into Bard, Pixel 8 Pro, and other Google products planned.\n",
      "   - Benchmark performance surpassed competitors, with Ultra exceeding human experts on the MMLU test.\n",
      "   - Initial release limited to English due to ongoing safety testing.\n",
      "\n",
      "4. Evolution and Expansion (2024-2025):\n",
      "   - Partnership with Samsung for Galaxy S24 integration.\n",
      "   - Bard and Duet AI unified under the Gemini brand.\n",
      "   - Gemini 1.5 released with increased power and context window.\n",
      "   - Gemma, an open-source, lightweight version, launched.\n",
      "   - Gemini 1.5 Flash introduced.\n",
      "   - Gemini 2.0 Flash Experimental released with enhanced multimodal capabilities, tool integration, and improved performance, later becoming the default.\n",
      "   - Gemini 2.0 Pro released.\n",
      "\n",
      "5. Architecture and Focus:\n",
      "   - Based on decoder-only transformers, optimized for TPUs.\n",
      "   - Utilizes a multimodal, multilingual dataset.\n",
      "   - Features a flexible context window handling various data types in any order.\n",
      "   - Different model generations and sizes cater to various needs, from cloud-based to on-device applications.\n",
      "\n",
      "6. Responsible Development:\n",
      "   - Google emphasizes responsible AI development, prioritizing safety testing.\n",
      "   - Collaboration with governments on AI safety is a key focus. \n"
     ]
    }
   ],
   "source": [
    "print(summary[\"output_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c326e-bada-4a79-b734-eedef7aae07e",
   "metadata": {},
   "source": [
    "### Refine with Custom template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "65375268-0993-43ad-b283-9a3122f8d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Please summary of the following:\n",
    "{text}\n",
    "Summary:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\"\"\"\n",
    "    Please provide a final summary of the text with these important points\n",
    "    Summary: `{text}`\n",
    "    \"\"\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "summary = chain.invoke(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "27c0b01c-645e-405b-b94b-88ff23aae14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini is Google's family of multimodal large language models (LLMs), developed by Google DeepMind to succeed LaMDA and PaLM 2.  Launched in December 2023, it competes with OpenAI's GPT-4 and powers Google's chatbot (also named Gemini).  Key features include its multimodal capabilities (processing text, images, audio, video, and code), its development by the merged Google DeepMind team, and its training on a massive, diverse dataset including YouTube transcripts.\n",
      "\n",
      "Gemini has several versions: Ultra (most powerful), Pro (wide range of tasks), Flash (faster, on-device tasks), and Nano (for smartphones).  It has achieved benchmark performance exceeding GPT-4 and other LLMs, even surpassing human experts on the MMLU test.  Gemini has been integrated into various Google products like Bard, Pixel phones, Search, and Google Cloud services.\n",
      "\n",
      "The model has seen continuous updates, with Gemini 1.5 introducing a larger context window and improved architecture. Google also released Gemma, an open-source, lightweight version of Gemini.  Gemini 2.0 Flash, launched in late 2024, further enhanced real-time audio/video interactions, spatial understanding, and native image generation.  It also introduced improved agent capabilities and a new Google Gen AI SDK.  Google continues to refine and expand Gemini's capabilities, making it a central part of their AI strategy.\n"
     ]
    }
   ],
   "source": [
    "print(summary[\"output_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c7cd04-81cc-4dd8-af47-323def68198a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
